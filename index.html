<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-122759872-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        
        gtag('config', 'UA-122759872-1');
        </script>
    
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link rel="shortcut icon" HREF="index/favicon.ico">
            <title>Libing Zeng</title>
            <link rel="stylesheet" type="text/css" href="./index/main.css">
                <link href="./index/css" rel="stylesheet">
                    <style>
                        .quote{
                            font-family: 'Dawning of a New Day';
                            font-weight:bold;
                            font-size:30px;
                        }
                    </style>
                     
</head>

<body>
    <div id="main">
        
        <div class="name">
            Libing Zeng (曾立兵)
        </div>
        
        <div>
            <img src="./index/libingzeng.jpg" style="width:205px; height:256px" class="portrait"><br>
            <!-- <img src="./index/libingzeng2.jpg" style="width:256px; height:256px" class="portrait"><br> -->
                &nbsp;&nbsp;&nbsp;
                
                <div class="bio_format">
 
                    <p> 
                        I am a final-year PhD candidate, 
                        supervised by Dr. <a href="http://faculty.cs.tamu.edu/nimak/">Nima Khademi Kalantari</a>, 
                        in <a href="https://engineering.tamu.edu/cse/index.html">Computer Science and Engineering</a> department at <a href="https://www.tamu.edu/">Texas A&M University</a>. 
                        I am also a member in the <a href="https://aggie.graphics/">Aggie Graphic Group</a>.
                        I have had the privilege of working closely with Dr. <a href="http://www.liyiwei.org/">Li-Yi Wei</a> and Dr. <a href="https://www.pauldebevec.com/">Paul Debevec</a> during my academic journey.
                        I earned my bachelor's degree in Electronic Information Engineering from <a href="http://www-en.hnu.edu.cn/">Hunan University</a> where I worked with Dr. <a href="http://eeit.hnu.edu.cn/info/1411/4632.htm">Shaoyuan Wang</a>.
                    </p>

                    <p> 
                        My current research focuses on leveraging deep learning techniques to solve challenges in computer graphics and computer vision, 
                        with a particular emphasis on <b style="color: #800000">Facial Image Editing</b>.
                        My goal is to  bridge artistic creativity with technical precision, advancing realistic, controllable, and personalized solutions in digital human technologies.
                    </p>

                    <!-- Here is my <a href="index/cv_libingzeng.pdf">CV</a>&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=1BKSlU8AAAAJ&hl=en" target="blank"><img src="index/google_scholar_logo.png" width="25px"></a>&nbsp;
                    <a href="https://www.linkedin.com/in/libing-zeng-88640832/" target="blank"><img src="index/linkedin_logo.png" width="25px"></a>&nbsp;
                    <a href="https://github.com/libingzeng" target="blank"><img src="index/github_logo.png" width="25px"></a>&nbsp;
                    <a href="mailto:libingzeng@tamu.edu"><img src="index/gmail_logo.png" width="25px"></a>&nbsp;
                    <a href="https://blog.csdn.net/libing_zeng" target="blank"><img src="index/csdn_logo.png" width="25px"></a>&nbsp;
                </div>
        </div>
        
        
        <div>
            <span class="category">News</span>
            <hr class="line">
            <div align=left
                style='
                border: solid 2px grey;
                width: 1000px;
                height: 150px;
                overflow: scroll;
                scrollbar-face-color: #889B9F;
                scrollbar-shadow-color: #3D5054;
                scrollbar-highlight-color: #C3D6DA;
                scrollbar-3dlight-color: #3D5054;
                scrollbar-darkshadow-color: #85989C;
                scrollbar-track-color: #95A6AA;
                scrollbar-arrow-color: #FFD6DA;
                '>
                
                <ul class="item_content">
                    <li>
                        <font color="black"><span>[10/2024] One first-authored paper accepted to <b>WACV 2025</b>.</span></font>
                        <!-- <font color="red"><sup class="highlight">New</sup></font> -->
                    </li>
                    <li>
                        <font color="black"><span>[01/2024] Incoming research intern collaborating with Dr. <a href="https://www.pauldebevec.com/"><b>Paul Debevec</b></a> at <a href="https://www.eyelinestudios.com/">Eyeline Studios - Powered by Netflix</a>.</span></font>
                    </li>
                    <li>
                        <font color="black"><span>[08/2023] One first-authored paper conditionally accepted to <b>SIGGRAPH Asia 2023</b>.</span></font>
                        <!-- <font color="red"><span>[08/2023] One first-authored paper conditionally accepted to <b>[&#9733 SIGGRAPH Asia 2023 &#9733]</b>.</span></font> -->
                        <!-- <font color="red"><sup class="highlight">New</sup></font> -->
                    </li>
                    <li>
                        <font color="black"><span>[07/2023] One co-authored paper accepted to <b>ICCV 2023</b>.</span></font>
                        <!-- <font color="red"><sup class="highlight">New</sup></font> -->
                    </li>
                    <li>
                        <font color="black"><span>[02/2023] One first-authored paper accepted to <b>CVPR 2023</b>.</span></font>
                    </li>
                    <li>
                        <font color="black"><span>[12/2022] One first-authored paper accepted to <b>CGF 2023 (Eurographics 2023)</b>.</span></font>
                    </li>
                    <li>
                        <font color="black"><span>[05/2021] One co-authored paper accepted to <b>ICCP 2021</b>.</span></font>
                    </li>
                    <!-- <li>
                        <font color="black"><span>[08/2019] I start my Ph.D. study in TAMU.</span></font>
                    </li>
                    <li>
                        <font color="black"><span>[03/2019] I get a Ph.D. offer in computer graphics from UCSB.</span></font>
                    </li>
                    <li>
                        <font color="black"><span>[02/2019] I get a Ph.D. offer in computer graphics from TAMU.</span></font>
                    </li>
                    <li>
                        <font color="black"><span>[12/2018] I am looking for Ph.D. position in computer graphics in the United States.</span></font>
                    </li> -->
                </ul>

            </div>
        </div>
       

        <div>
            <span class="category">Publications</span>
            <hr class="line">
            <ol style="padding:0px;list-style-type:none">

                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/egocentric.gif" class="img_format"> -->
                    <img src="./research/relighting_representative.jpg" class="img_format">
                        <div class="title_format">
                            <b>Analyzing and Improving the Skin Tone Consistency and Bias in Implicit 3D Relightable Face Generators</b>
                            <span class="author_format"><b style="color: #800000">Libing Zeng</b>, Nima Khademi Kalantari.</span>
                            <span class="venue_format"><em style="color: #008B8B">The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2025) </em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="In this paper, We have presented a comprehensive analysis and solution to the problem of the skin tone inconsistency and bias in the relightable face generator with implicit lighting representation. ">Abstract</a>]
                                [<a href="https://arxiv.org/pdf/2411.12002">Paper</a>]
                                [<a href="projects/relighting/relighting.html">Project</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                        
                </li>

                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/egocentric.gif" class="img_format"> -->
                    <img src="./research/mystyle_representative.jpg" class="img_format">
                        <div class="title_format">
                            <b>MyStyle++: A Controllable Personalized Generative Prior</b>
                            <span class="author_format"><b style="color: #800000">Libing Zeng</b>, Lele Chen, Yi Xu, Nima Khademi Kalantari.</span>
                            <span class="venue_format"><em style="color: #008B8B">SIGGRAPH Asia 2023</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="In this paper, we propose an approach to obtain a personalized generative prior with explicit control over a set of attributes.">Abstract</a>]
                                [<a href="https://arxiv.org/abs/2306.04865/">Paper</a>]
                                [<a href="projects/mystyle++/mystyle++.html">Project</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                        
                </li>

                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/egocentric.gif" class="img_format"> -->
                    <img src="./research/egocentric.jpg" class="img_format">
                        <div class="title_format">
                            <b>Uncertainty-aware State Space Transformer for Egocentric 3D Trajectory Forecasting</b>
                            <span class="author_format">Wentao Bao, Lele Chen, <b style="color: #800000">Libing Zeng</b>, Zhong Li, Yi Xu, Junsong Yuan, Yu Kong.</span>
                            <span class="venue_format"><em style="color: #008B8B">The International Conference on Computer Vision (ICCV 2023)</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="In this paper, we set up an egocentric 3D hand trajectory forecasting task that aims to predict hand trajectories in a 3D space from early observed RGB videos in a first-person view.">Abstract</a>]
                                [<a href="https://browse.arxiv.org/pdf/2307.08243.pdf">Paper</a>]
                                [<a href="https://actionlab-cv.github.io/EgoHandTrajPred/">Project</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                        
                </li>
                
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/landmark_detection.gif" class="img_format"> -->
                    <img src="./research/landmark_teaser.png" class="img_format">
                        <div class="title_format">
                            <b>3D-aware Facial Landmark Detection via Multiview Consistent Training on Synthetic Data</b>
                            <span class="author_format"><b style="color: #800000">Libing Zeng</b>, Lele Chen, Wentao Bao, Zhong Li, Yi Xu, Junsong Yuan, Nima Khademi Kalantari.</span>
                            <span class="venue_format"><em style="color: #008B8B">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2023)</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="In this paper, by leveraging synthetic data, we propose a novel multi-view consistent learning strategy to improve 3D facial landmark detection accuracy on in-the-wild images. The proposed 3D-aware module can be plugged into any learning-based landmark detection algorithm.">Abstract</a>]
                                [<a href="projects/landmark/landmark/landmark_cvpr.pdf">Paper</a>]
                                [<a href="projects/landmark/landmark.html">Project</a>]
                                <!-- [<a href="https://people.engr.tamu.edu/nimak/Papers/CVPR2023_Landmark/index.html">Project</a>] -->
                                 </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>
                        
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/depth_estimation.png" class="img_format" style="width:550px;height:300px;"> -->
                    <img src="./research/depth_median_confidence2.png" class="img_format">
                        <div class="title_format">
                            <b>Test-Time Optimization for Video Depth Estimation Using Pseudo Reference Depth
                            </b>
                            <span class="author_format"><b style="color: #800000">Libing Zeng</b>, Nima Khademi Kalantari.</span>
                            <span class="venue_format"><em style="color: #008B8B">Computer Graphics Forum (CGF 2023) (Proceedings of Eurographics 2023)</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="In this paper, we propose a learning-based test-time optimization approach for reconstructing geometrically consistent depth maps from a monocular video. Specifically, we optimize an existing single image depth estimation network on the test example at hand. We do so by introducing pseudo reference depth maps which are computed based on the observation that the optical flow displacement for an image pair should be consistent with the displacement obtained by depth-reprojection. Additionally, we discard inaccurate pseudo reference depth maps using a simple median strategy and propose a way to compute a confidence map for the reference depth. We use our pseudo reference depth and the confidence map to formulate a loss function for performing the test-time optimization in an efficient and effective manner. ">Abstract</a>]
                                [<a href="research/depth_cgf.pdf">Paper</a>]
                                [<a href="projects/depth/depth.html">Project</a>]
                                <!-- [<a href="https://people.engr.tamu.edu/nimak/Papers/ICCP2021_denoising/index.html">Project</a>]
                                [<a href="https://arxiv.org/abs/2103.02861">Paper</a>]
                                [<a href="https://github.com/avinashpaliwal/MaskDnGAN">Code</a>]
                                [<a href="https://www.youtube.com/watch?v=PXbYS4AoJiQ&t=5402s">Talk</a>] -->
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/iccp2021_denoising.png" class="img_format" style="width:550px;height:300px;"> -->
                    <img src="./research/iccp2021_denoising.png" class="img_format">
                        <div class="title_format">
                            <b>Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask</b>
                            <span class="author_format">Avinash Paliwal, <b style="color: #800000">Libing Zeng</b>, Nima Khademi Kalantari.</span>
                            <span class="venue_format"><em style="color: #008B8B">International Conference on Computational Photography (ICCP 2021)</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="In this paper, we propose a learning-based approach for denoising raw videos captured under low lighting conditions. We propose to do this by first explicitly aligning the neighboring frames to the current frame using a convolutional neural network (CNN). We then fuse the registered frames using another CNN to obtain the final denoised frame. To avoid directly aligning the temporally distant frames, we perform the two processes of alignment and fusion in multiple stages. ">Abstract</a>]
                                [<a href="https://arxiv.org/abs/2103.02861">Paper</a>]
                                [<a href="projects/denoise/denoise.html">Project</a>]
                                <!-- [<a href="https://people.engr.tamu.edu/nimak/Papers/ICCP2021_denoising/index.html">Project</a>] -->
                                <!-- [<a href="https://github.com/avinashpaliwal/MaskDnGAN">Code</a>] -->
                                [<a href="https://www.youtube.com/watch?v=PXbYS4AoJiQ&t=5402s">ICCP Talk</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>
                 
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/pfmlt.jpg" class="img_format" style="width:550px;height:300px;"> -->
                    <img src="./research/pfmlt.jpg" class="img_format">
                        <div class="title_format">
                            <b>Rectifying Proposal Failures in Metropolis Light Transport</b>
                            <span class="author_format"><b style="color: #800000">Libing Zeng</b>, Li-Yi Wei.</span>
                            <span class="venue_format"><em style="color: #008B8B">Preprint (HAL 2019)</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="Based on MMLT, we propose a novel algorithm, Rectifying Proposal Failure MLT (RPFMLT), which distinguishes proposal failure paths from normal proposed paths and excludes them from the states of Markov chain. PFMLT better approximates the original path distributions especially for high proposal rejection rates, and can be easily integrated with various MLT algorithms.">Abstract</a>]
                                [<a href="https://hal.archives-ouvertes.fr/hal-02548744">Paper</a>]
                                [<a href="https://github.com/libingzeng/pbrt-v3">Code_pbrt</a>]
                                [<a href="https://github.com/libingzeng/tungsten">Code_tungsten</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>

            </ol>
        </div>

        
        <div>
            <span class="category">Projects (Prior to Ph.D.)</span>
            <hr class="line">
            <ol style="padding:0px;list-style-type:none">
                
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/bart_robot1.jpg" class="img_format" style="width:550px;height:300px;"> -->
                    <img src="./research/bart_robot1.jpg" class="img_format">
                        <div class="title_format">
                            <b>A Renderer Written from the Scratch (BART Animations with High-Frequency Textures)</b>
                            <span class="author_format">Supervised by Dr. Li-Yi Wei.</span>
                            <span class="venue_format"><em style="color: #008B8B">Exercises</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="This renderer mainly has two components. First, parsing animation description files (AFF) without using flex and bison; Second, rendering the three animations of BART with much less artifacts than the original ones. Some key implementations for reducing artifacts: Multi-Jittered sampler, Gaussian filter for reconstruction, Mipmap and EWA filter for texture anti-aliasing. ">Abstract</a>]
                                [<a href="https://github.com/libingzeng/BART-Animations">Project</a>]
                                [<a href="https://youtu.be/yfTIEGcqGfU">Animation_robot</a>]
                                [<a href="https://youtu.be/2veK2uj-yFk">Animation_kitchen</a>]
                                [<a href="https://youtu.be/U0Dylgw_qUk">Animation_museum</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/rtgu_global.jpg" class="img_format" style="width:550px;height:300px;"> -->
                    <img src="./research/rtgu_global.jpg" class="img_format">
                        <div class="title_format">
                            <b>A Renderer Extended from the Book, Ray Tracing from the Ground Up</b>
                            <span class="author_format">Self-study</span>
                            <span class="venue_format"><em style="color: #008B8B">Exercises</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="He read the book and extended it from the following aspects. Objects: part of sphere, part of tori, globe, and so on. Tessellation: sphere, horn, rotational sweeping surfaces, bezier patches (Utah teapot). 2D texture: solid cylinder checker, sphere checker. Scene building: several scenes were built and the corresponding resultant images were produced.">Abstract</a>]
                                [<a href="https://github.com/libingzeng/RayTraceGroundUp">Project</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>
                
                <li class="item_format" style="position:relative">
                    <!-- <img src="./research/nontriangulated_surfaces.png" class="img_format" style="width:550px;height:300px;"> -->
                    <img src="./research/nontriangulated_surfaces.png" class="img_format">
                        <div class="title_format">
                            <b>A Renderer with Implementations of Various NON-TRIANGULATE Surfaces</b>
                            <span class="author_format">Self-study</span>
                            <span class="venue_format"><em style="color: #008B8B">Exercises</em></span>
                            <div class="link_format">
                                [<a class="btn btn-default abstract" abstract="This render is based on the framework of Peter Shirley's “ray tracing in one weekend” and is extended with tracing almost all of the NON-TRIANGULATE surfaces mentioned in the book, An Introduction to Ray Tracing. ">Abstract</a>]
                                [<a href="https://github.com/libingzeng/AnIntroductionToRayTracing">Project</a>]
                            </div>
                        </div>
                        <div style="clear:both;"></div>
                </li>
 
            </ol>
        </div>
        
        
        <div>
            <span class="category">Teaching</span>
            <hr class="line">
            <ul class="item_content">
                <li>
                    <a href="https://people.engr.tamu.edu/nimak/Courses/CompPhoto/Spring2025/index.html"> 2025 SPRING CSCE 448/748: COMPUTATIONAL PHOTOGRAPHY</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/nimak/index.html"> Nima Khademi Kalantari </a></span>
                </li>
                <li>
                    <a href="https://people.engr.tamu.edu/nimak/Courses/CSCE441-CompGraph/Fall2024/index.html"> 2024 FALL CSCE 441: COMPUTER GRAPHICS</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/nimak/index.html"> Nima Khademi Kalantari </a></span>
                </li>
                <li>
                    <a href="https://people.engr.tamu.edu/nimak/Courses/CompPhoto/Spring2024/index.html"> 2024 SPRING CSCE 448/748: COMPUTATIONAL PHOTOGRAPHY</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/nimak/index.html"> Nima Khademi Kalantari </a></span>
                </li>
                <li>
                    <a href="https://people.engr.tamu.edu/nimak/Courses/CSCE441-CompGraph/Fall2023/index.html"> 2023 FALL CSCE 441: COMPUTER GRAPHICS</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/nimak/index.html"> Nima Khademi Kalantari </a></span>
                </li>
                <li>
                    <a href="https://classroom.google.com/u/0/c/NTQxMTcwOTA2NzQ2"> 2023 SPRING CSCE 482: SENIOR CAPSTONE DESIGN</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/dzsong/index.html"> Dezhen Song </a></span>
                </li>
                <li>
                    <a href="https://people.engr.tamu.edu/sueda/courses/CSCE441/2022S/index.html"> 2022 SPRING CSCE 441: COMPUTER GRAPHICS</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/sueda/index.html"> Shinjiro Sueda </a></span>
                </li>
                <li>
                    <a href="https://people.engr.tamu.edu/djimenez/classes/312/index.html"> 2021 FALL CSCE 312: COMPUTER ORGANIZATION</a>
                    <span class="author_format">Instructor: Dr. <a href="https://people.engr.tamu.edu/djimenez/index.html"> Daniel A. Jiménez </a></span>
                </li>
            </ul>
        </div>
        

        <div>
            <span class="category">Community Services</span>
            <hr class="line">
            <ul class="item_content">
                <li>
                    <p class="text-justify">
                        SIGGRAPH Asia: Reviewer 2024.
                    </p>
                </li>           
                <li>
                    <p class="text-justify">
                        ECCV: Reviewer 2024.
                    </p>
                </li>           
                <li>
                    <p class="text-justify">
                        ICLR: Reviewer 2025.
                    </p>
                </li>           
                <li>
                    <p class="text-justify">
                        WACV: Reviewer 2024/2025.
                    </p>
                </li>                
                <li>
                    <p class="text-justify">
                        ICPR: Reviewer 2024.
                    </p>
                </li>           
                <li>
                    <p class="text-justify">
                        CVIU: Reviewer 2024.
                    </p>
                </li>           
            </ul>
        </div>


        <div>
            <span class="category">Awards</span>
            <hr class="line">
            <ul class="item_content">
                <li>
                    <p class="text-justify">
                        Travel Grant, Department of Computer Science and Engineering, Texas A&M University, Spring 2025
                    </p>
                </li>           
                <li>
                    <p class="text-justify">
                        Travel Grant, Department of Computer Science and Engineering, Texas A&M University, Fall 2023
                    </p>
                </li>           
                <li>
                    <p class="text-justify">
                        Travel Grant, Department of Computer Science and Engineering, Texas A&M University, Spring 2023
                    </p>
                </li>           

            </ul>
        </div>

        
        <div>
            <a href="https://clustrmaps.com/site/1ax9f"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=2VYrG4d0yS6ggLr46bOONoSDAi04lXmW64rQVnzoZlE&cl=ffffff" /></a>
        </div>

    </div>
    

    <script src="./index/canvas-nest.js_1.0.1_canvas-nest.min.js" opacity="0.6" color="0,68,255" zindex="-1"></script><canvas id="c_n1" width="1287" height="736" style="position: fixed; top: 0px; left: 0px; z-index: -1; opacity: 0.6;"></canvas>

</body></html>
